# 问题1：实例概览数据异常

## 问题描述

运维中心-运维概览页面任务实例总数为0。

## 问题排查

### 调度日志

由于调度每天23:30进行实例化，查看13号调度日志如下，发现是调用资源管理超时失败：

<img src="xingye/image-20210416115230467.png" alt="image-20210416115230467" style="zoom:50%;" />

接上一张图的堆栈，超时报错

![image-20210416115557475](/Users/renfei/Desktop/xingye/image-20210416115557475.png)

### 查看资源管理日志及线程

现象1：资源管理无full gc情况，资源管理网络连接无异常；资源管理服务可用、部分接口响应较慢。

现象2：查看资源管理日志，发现资以下源管理日志不断刷屏：

```shell
# 日志关键信息
..."收到获取项目下某些类型的数据源"...
# 对应接口
RemoteStorageServiceImpl#getProjectStorageList
```

观察资源管理线程，发现tomcat一直长期占用cpu资源，这跟离线200个tomcat线程被打满（见离线日志分析）且线程堆栈都是调用资源管理接口的情况相对应；

结论：由于离线大量占用资源管理资源导致请求堆积太多进，导致调度在晚上11点在调用getStorageListContainsClusterService接口超时最终导致实例化失败。

![image-20210416120240939](xingye/image-20210416120240939.png)



### 离线日志

jstack查看离线日志，统计堆栈数量200+，这与默认的tomcat线程数相同，一方便说明batch-server线程资源被占满；同时按照**核数:线程池数量**的调优比例来看，线上tomcat线程数比较低。

通过线程栈可以看出离线这边一直在做资源管理接口的访问，处理其他外部请求大概率可能会比较慢。

<img src="/Users/renfei/Desktop/xingye/image-20210416121123542.png" alt="image-20210416121123542" style="zoom:50%;" />



## 问题解决方案

### 调优方案

因为资源管理服务并非不可用只是响应较慢，观察到实例化失败是6s超时，所以这里改成60s；

同时基于16 core的配置线程并发量适当调大为1024.

```shell
# dubo超时时间
dubbo.provider.timeout=6000 -> 60000

# 增加资源管理线程并发量
dubbo.protocol.threads=200 -> 1024
```

### 缓存方案

修改资源管理代码，添加缓存方案。

#### Spring Cache/caffeine

缓存策略：

配置文件添加如下配置：

```java
# spring cache
spring.cache.cache-names=storageDetail,storageDetailList,projectStorageList,clusterServiceList
spring.cache.caffeine.spec=maximumSize=500,expireAfterAccess=600s
```

#### 缓存接口列表

兴业缓存的接口列表：

```java
@Cacheable(value = "projectStorageList", key = "#projectStorageReq.projectId")
public List<DataStorageDTO> getProjectStorageList(ProjectStorageReq projectStorageReq)
  
@Cacheable(value = "storageDetail", key = "#storageDetailReq.storageId")
public StorageDTO getDetail(StorageDetailReq storageDetailReq)
  
@Cacheable(value = "storageDetailList")
public List<StorageDTO> getStorageList(StorageDetailReq storageDetailReq)

@Cacheable(value = "clusterServiceList")
public Map<String, List<ClusterServiceInfoDTO>> getClusterServiceListByClusterIds(Long tenantId, List<String> clusterIds)

```

# 问题2：spark端口占用

## 问题描述

SparkSQL作业报:

```
Address already in use: service 'sparkDriver' failed after 256 retries
```

## 问题排查

由于spark作业是基于spark client的方式进行提交的，所以首先应该排查执行代理节点的端口占用情况；

**排查代理节点**

 分别在三个执行代理节点，netstat查看网络连接异常，发现在63节点出现大量close_wait，netstat查看有接近9w左右的close_wait，lsof -i:1004查看为datanode服务。

由于在这之前手动触发过datax任务，并且停止datax任务后连接得以释放，所以基本确定是datax任务导致。

**问题结论**

由于datax大量占用系统端口，导致spark作业无可用端口，导致256次重试失败。

datax任务:

从hive同步作业到es。

**排查代码**

内研同学排查代码发现是hdfs.open方法打开新的流后没有释放，这种情况在少量请求下可能无法暴露问题。

## 问题解决方案

a.内研修改datax代码。

b.修改以下参数：

```
spark.port.maxRetries=256

spark.blockManager.port=30000
```

# 问题记录

## 系统问题

```
从这几次定位的问题来看，修改一些常见的系统参数比如fd是非常重要的，特别的大并发情况下
```

## 平台调度策略

在多个执行代理ha的情况下，平台调度策略解决不了一台执行代理所在服务器资源（比如fd、port等）不可用的情况，
所以在这种情况下，其他执行代理节点资源没有充分利用；即使有重复策略也无法保证第二次不会调用到有问题的节点。

## dubbo服务治理

从目前问题来看，关键服务如果不做限流/降级（这需要对单台服务TPS有所了解），在兴业这种并发量比较大的场景会出现多个服务级联挂掉或者卡顿的情况。

比如batch-server大量请求resource-server，resource-server如果不对消费者端限流或者降级，调度服务去获取资源管理服务时也会受到影响（比如time-out）。

https://blog.csdn.net/u012965203/article/details/98253914

# 参考资料

https://spring.io/guides/gs/caching/

https://blog.csdn.net/u012965203/article/details/98253914





